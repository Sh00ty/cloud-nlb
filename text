- 1 слайд:

Здравствуйте уважаемая комиссия, перед вами выступает студент группы М05-412в 
Шлыков Павел. И тема моей НИР - Реализация отказоустойчивого многопользовательского 
L3/L4 балансировщика нагрузки на базе VPP. Научный руководитель - Жданов Яков, 
СТО направления network в MWS облаке.


- 2 слайд:

Перед тем как приступать к рассмотрению требований к балансировщику нагрузки в 
облаке рассмотрим что такое сетевая балансировка нагрузки:
“Сетевая балансировка нагрузки - процесс распределения сетевого трафика 
(на различных уровнях модели OSI, обычно на 3-4-7) между двумя или более точками его обработки.”

В общем случае балансировщики помогают различного рода инфраструктуре предоставляя:
Повышение отказоустойчивости систем обработки сетевого трафика [L4/L7]
Снижение задержек в обработке запросов для пользователей [L4/L7]
Увеличение общей пропускной способности системы [L4/L7]
Увеличение утилизации ресурсов обработчиков трафика [L4/L7]
Защита от DoS атак [L7]
Возможность терминации TLS соединений [L7]

Есть еще некоторые приемущества, о которых не сказать нет возможности в силу ограниченного времени.


- 3 слайд:

Говоря о балансировщиках нагрузки в облаках хочется отметить, что: 
Обычно балансировщики нагрузки обслуживают весь входящий в облако клиентский трафик. 
В связи с этим к таким балансировщикам устанавлиются высокие требования, т.е. 
высокие SLA. (в районе 99.99 uptime)
Также ввиду требований к производительности и отказоустойчивости ставить L7 
балансировщик - слишком дорого и неэффективно для облачного провайдера.
Одним из главных требований к L3/L4 - консистентность соединений. Это значит, 
что все ip пакеты в рамках одной tcp/udp сессии должны приходиться на один и 
тот же узел обработки трафика.В случае нарушения консистентности таких сессий 
пользователи клиентских сервисов будут получать ошибки, что скажется на их SLA.


Вдобавок чтобы балансировщик нагрузки действительно повышал уровень сервиса в 
облаке, он должны направлять трафик только на “живые/работающие” узлыы обработки 
трафика. Это ключевое требование к системе балансировки.


Еще балансировщик нагрузки должен предоставлять абстракцию в виде виртуальных IP 
адресов (VIP), что является удобным API для пользователей облака.
Также учитывая специфику облачного провайдера:
- Балансировщик должен быть мультитенантный, и стараться ограничивать влияние 
одних пользователей на других (изоляция).
- Также пользователю в облаке должен предоставляться интерфейс для конфигурации 
балансировщика, так как это является частью LBaaS.


- 4 слайд:

Поговорим подробнее о виртуальных IP адресах. Это важная абстракция. На данном слайде представленно как это должно работать:

Клиент обращается к одному белому IP адресу. Но в реальности за VIP скрывается множество узлов обработки трафика уже с реальными IP адресами, что позволяет:


- 5 слайд:

В 2017 году google представил научную статью по тому, как работает балансировщик 
нагрузки, который они назвали maglev. 

Через bgp виртуальный ip адрес клиента публикуется в интернет
Благодаря ECMP протоколу запросы с роутера равномерно распределяются нагрузку на 
stateless балансировщики обслуживающие конкретные VIP.

В свою очередь балансировщики уже с помощью алгоритмов консистентного хешировать 
выбирают для каждой пятерки (ip, port источник, ip port приемника и протокол) выбирается
конкретный узел для обработки.

Так как consistent-hashing state-less протокол - это позволяет при отказе нод 
балансировщиков принимать такие же решения на других нодах и не рвать соединения,
что очень важно.

Также преимуществами схемы являются:
Избыточность и запас устойчивости N+1.
Обработка одного VIP на множестве балансировщиков в отличии от других популярных
схем балансировки
Возможность шардирования VIP.
Возможность построения “больших ферм” балансировщиков.
Используется Direct Server Return - благодаря которому ответы, которые зачастую
по размеру больше запросов возвращаются мимо балансировщика


- 6 слайд:

Таким образом актуальность выглядит следующим образом:

Балансировщик нагрузки является ключевой сущностью в облаке.
Аналогичные Google Maglev решения существуют, но детали их реализации не разглашаются.
В open-source нет аналогов, отвечающих всем требованиям балансировщика в облаке.
Реализация зависит от специфики облачного провайдера.


- 7 слайд:

--------------

- 8 слайд:

--------------

- 9 слайд:

А теперь перейдем к рассмотрению принципиальной архитектуры балансировщика, 
которую необходимо будет имплементировать.

На данном слайде изображена архитектура балансировщика, похожая на архитектуру
google maglev, но с учетом взаимодействия сервисов control-plane, health controller
и health-check-node и lb_agent которые не освещались в данной статье и которые
придется реализовывать в данной работе.

Рассмотрим за что отвечает каждый из компонентов:

В данной архитектуре за принятие всех решение о публикации VIP с помощью BGP,
о процессе вывода балансировщика или RIP из под нагрузки, принимает компонент,
называющийся control plane. 
Это своего рода мозг балансировщика.

“руками”, балансировщика будет являться lb agent, запускаемый непосредственно
вместе с инстансом vpp на узле. Он будет отвечать за проверку состояния узла
балансировщика и самого vpp. 
Также он будет отвечать за конфигурацию VPP. А VPP в свою очередь остается
заниматься лишь балансировкой нагрузки.

Последним по очереди, но не по важности будет health-check node и health-controller. 
Health-controller будет отвечать за организацию процесса сбора проверок состояния 
клиентских адресов. 

Это является также непростой задачей, так как отсутствие своевременной проверки
состояния приведет к нарушению SLA в облаке, так как балансировщик будет проксировать
запросы на нерабочие узлы.
Также данный сервис будет хранить данные о том, каким именно образом опрашивать
клиентские сервисы и назначить задачи на опрос конкретным hc-node.

Непосредственным опросом клиентских сервисов по различным протоколам (tcp/http/grpc)
будет заниматься health-check-node.

Таким образом соединив все компоненты воедино и получается облачный балансировщик.


- 10 слайд:

Перейдем к рассмотрению ключевой технологии для data-plane в облаках.

Выбор выпал на VPP именно из-за:

Гибкой и расширяемой архитектуры, которая позволяет встраивать плагины с 
произвольной логикой

Бенчмарки проводимые в google и vpp показывают, что балансировщики с kernel
bypass могут привосходить конкурентов без него на порядки

Существует плагин, который работает схожим образом с релизацией maglev от goolge:
те использование maglev-hash

Вдобавок в проекте активно используются simd операции и оптимизации под L1/L2 кеши,
что еще сильнее ускоряет данный балансировщик

Таким образом vpp является одним из самых производительных и надежных решений на
open-source рынке.


- 11 слайд:

На данный момент в рамках работы был полностью спроектирован и реализован модуль проверок состояния и отправки изменений их статуса в control-plane.

На данном слайде изображена архитектура данного модуля, обсуждать все нюансы не позволяет ограничения по времени. 

Но пройдем по ключевым-особенностям, выделяющим данное решение от остальных:

health-worker-node запускается в k8s в statefull set для поддержки высокого
уровня горизонтальной масштабируемости.

Детектирование сбоев подов health-worker-node с помощью децентрализованного
gossip протокола swim.

Так как проверок состояний в системе ожидает много, то их распределение между
нодами осуществляется с помощью двухуровневого consistent-hashing.
Таким образом эти решения позволяют системе горизонтально масштабироваться и быть
отказоустойчивой. 

Потенциальными точками отказа являются sql-база данных и redpanda очереди, но в
облаке уже есть отказоустойчивые реализации данные примитивов, поэтому эта проблема
практически не существенна.


- 12 слайд:

В рамках работы была проведена серия экспериментов на локальном узле: были сэмулированы задержки между подами в statefull-set и задержки до базы данных. Также было вставлено 5000 проверок-заглушек и несколько контрольных реальных проверок относительно которых мерились метрики простоя системы. 

В результате эксперементов удалось выявить:
Горизонтальное масштабирование узлов вообще не вызывает простоев в работе
При эмуляции множественного отказ подов, удалось выявить, что простой при отказах составляет не более 6 секунд, что позволяет укладывать в SLA облака 99.99%  даже при ежедневном сбое нод для проверок состояний.
При добавлении проверок состояний в процессе работы системы через cdc, было выявлено, что достака событий работает корректно, даже в процессе перестроения кластера.
То же самое было выявлено для обновления статусов для клиентских эндпоинтов. 

К добавок к вышесказоннному было выявлено, что, что планировщик может поддерживать более 20 тысяч задач на планировку, но также было выявлено, что обработать все 20 тысяч задач с одного узла не получается ввиду медленности сетевого стэка linux


- 13 слайд:

Благодаря тестированию и пониманию слабых мест проделанной работы можно
предложить потенциальные улучшения, которые не влияют на факт реализации работы:

- 14 слайд: